{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5470f524-1497-44a6-9fc1-b1541f376791",
   "metadata": {},
   "source": [
    "# Clustering et word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab9df9-3737-4fb0-9c57-ae33615ac09c",
   "metadata": {},
   "source": [
    "Sur la base des éléments méthodologiques et des enseignements techniques présentés lors du cours théorique, il est demandé dans le cadre de ce TP :\n",
    "- d’effectuer un clustering des bulletins pour une décennie au choix et d’interpréter les résultats\n",
    "- d’entraîner un modèle word2vec sur l’ensemble des bulletins et d’explorer les relations entre vecteurs\n",
    "\n",
    "Pour ce faire, vous utiliserez différentes librairies Python vues au cours comme scikit-learn et gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efd957-74d6-41a4-9abe-ca82e914a281",
   "metadata": {},
   "source": [
    "## 1. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82706675-c1cc-44fe-a1d7-4e02d87cb974",
   "metadata": {},
   "source": [
    "## 2. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6fcd20-ae63-448b-96bf-9f0d8c9ddfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cf288-0c22-490f-b93d-ede6d71fb3e3",
   "metadata": {},
   "source": [
    "#### Création d'un streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45027d3-067a-4306-9d66-232171f50c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cd2e3-4e7d-4755-a90d-2c7886a5f6ae",
   "metadata": {},
   "source": [
    "#### Fichier à analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590a72f4-6aa6-4e4c-9551-f2bb88a7650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/sents_2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0dd65-c5e7-49da-80b1-986f2272731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_temp = stopwords.words(\"french\")\n",
    "sw_temp += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb49831-c881-411f-b73e-f43527a784d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(my_file):\n",
    "    output_path = f\"../data/sents_2_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(my_file, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        kword = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.lower() for w in kword if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836b699-f741-4f82-a4bb-9950fb3a542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd2f49-000f-4fc4-a4e5-d2aa0e1afa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_infile = f\"../data/sents_2_clean.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce6a48-d5ed-4448-8388-9dc85d21e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(cleaned_infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f31cb3-5038-4f4d-bbab-c02ffc61c8e2",
   "metadata": {},
   "source": [
    "#### Création des bigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f866c305-7835-49f4-9ddd-3083f1ed1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d3d6e4-3ad3-4034-90e5-076077cff7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5978799"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e853e65e-afd4-4763-a237-b3a2fff6ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absence_&\n"
     ]
    }
   ],
   "source": [
    "# exploration des bigrammes\n",
    "\n",
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c73010-0803-4155-9c02-98a534affa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score de la clé\n",
    "\n",
    "bigram_phrases.vocab[key_]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
